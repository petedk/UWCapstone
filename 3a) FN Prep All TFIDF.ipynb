{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add testing url body text to csv to train a model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peted\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, time, gc, csv, scipy, pickle, warnings\n",
    "\n",
    "# Use process state = Pass only isPass=True\n",
    "isPass=True\n",
    "row_cnt = 10000\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Notebook_start = time.time()\n",
    "\n",
    "folder_loc = 'dataset/Raw/'\n",
    "\n",
    "file_type = 'All' \n",
    "files = ['True.csv','Fake.csv','URL_True.csv','URL_Fake.csv']\n",
    "\n",
    "save_loc = f'{folder_loc}LIWC/Stemmed_{file_type}/'\n",
    "\n",
    "from LIWC_Methods import Custom_Methods \n",
    "CM = Custom_Methods()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# natural language tool kit\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Raw/True.csv\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title_url_min</th>\n",
       "      <th>body_url_min</th>\n",
       "      <th>title_url</th>\n",
       "      <th>body_url</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>process_state</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True_16554</td>\n",
       "      <td>Thailand kicks off sumptuous funeral of King B...</td>\n",
       "      <td>BANGKOK (Reuters) - Thailand on Wednesday mark...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thailand kicks off sumptuous funeral of king b...</td>\n",
       "      <td>thailand on wednesday marked the start of a l...</td>\n",
       "      <td>thailand kicks off sumptuous funeral of king b...</td>\n",
       "      <td>thailand on wednesday marked the start of a la...</td>\n",
       "      <td>thailand kick sumptuou funer king bhumibol adu...</td>\n",
       "      <td>thailand wednesday mark start lavish five day ...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True_1652</td>\n",
       "      <td>California sues Trump administration over bord...</td>\n",
       "      <td>SAN FRANCISCO (Reuters) - California filed a l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>california sues trump administration over bord...</td>\n",
       "      <td>california filed a lawsuit on wednesday over ...</td>\n",
       "      <td>california sues trump administration over bord...</td>\n",
       "      <td>california filed a lawsuit on wednesday over t...</td>\n",
       "      <td>california sue trump administr border wall</td>\n",
       "      <td>california file lawsuit wednesday trump admini...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True_15473</td>\n",
       "      <td>Egypt Western Desert attack exposes front outs...</td>\n",
       "      <td>CAIRO (Reuters) - A deadly attack on the polic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>egypt western desert attack exposes front outs...</td>\n",
       "      <td>a deadly attack on the police in egypt s west...</td>\n",
       "      <td>egypt western desert attack exposes front outs...</td>\n",
       "      <td>a deadly attack on the police in egypt s weste...</td>\n",
       "      <td>egypt western desert attack expos front outsid...</td>\n",
       "      <td>deadli attack polic egypt western desert claim...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True_9308</td>\n",
       "      <td>North Korea says Trump isn't screwy at all, a ...</td>\n",
       "      <td>SEOUL (Reuters) - North Korea has backed pres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>north korea says trump isn't screwy at all, a ...</td>\n",
       "      <td>north korea has backed presumptive u.s. repub...</td>\n",
       "      <td>north korea says trump isn t screwy at all a w...</td>\n",
       "      <td>north korea has backed presumptive u s republi...</td>\n",
       "      <td>north korea say trump isnt screwi wise choic p...</td>\n",
       "      <td>north korea back presumpt u republican nomine ...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True_13639</td>\n",
       "      <td>NATO says North Korea missile launch undermine...</td>\n",
       "      <td>BRUSSELS (Reuters) - North Korea s latest miss...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nato says north korea missile launch undermine...</td>\n",
       "      <td>north korea s latest missile launch undermine...</td>\n",
       "      <td>nato says north korea missile launch undermine...</td>\n",
       "      <td>north korea s latest missile launch undermines...</td>\n",
       "      <td>nato say north korea missil launch undermin in...</td>\n",
       "      <td>north korea latest missil launch undermin regi...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  True_16554  Thailand kicks off sumptuous funeral of King B...   \n",
       "1   True_1652  California sues Trump administration over bord...   \n",
       "2  True_15473  Egypt Western Desert attack exposes front outs...   \n",
       "3   True_9308  North Korea says Trump isn't screwy at all, a ...   \n",
       "4  True_13639  NATO says North Korea missile launch undermine...   \n",
       "\n",
       "                                                body  news_url  \\\n",
       "0  BANGKOK (Reuters) - Thailand on Wednesday mark...       NaN   \n",
       "1  SAN FRANCISCO (Reuters) - California filed a l...       NaN   \n",
       "2  CAIRO (Reuters) - A deadly attack on the polic...       NaN   \n",
       "3   SEOUL (Reuters) - North Korea has backed pres...       NaN   \n",
       "4  BRUSSELS (Reuters) - North Korea s latest miss...       NaN   \n",
       "\n",
       "                                       title_url_min  \\\n",
       "0  thailand kicks off sumptuous funeral of king b...   \n",
       "1  california sues trump administration over bord...   \n",
       "2  egypt western desert attack exposes front outs...   \n",
       "3  north korea says trump isn't screwy at all, a ...   \n",
       "4  nato says north korea missile launch undermine...   \n",
       "\n",
       "                                        body_url_min  \\\n",
       "0   thailand on wednesday marked the start of a l...   \n",
       "1   california filed a lawsuit on wednesday over ...   \n",
       "2   a deadly attack on the police in egypt s west...   \n",
       "3   north korea has backed presumptive u.s. repub...   \n",
       "4   north korea s latest missile launch undermine...   \n",
       "\n",
       "                                           title_url  \\\n",
       "0  thailand kicks off sumptuous funeral of king b...   \n",
       "1  california sues trump administration over bord...   \n",
       "2  egypt western desert attack exposes front outs...   \n",
       "3  north korea says trump isn t screwy at all a w...   \n",
       "4  nato says north korea missile launch undermine...   \n",
       "\n",
       "                                            body_url  \\\n",
       "0  thailand on wednesday marked the start of a la...   \n",
       "1  california filed a lawsuit on wednesday over t...   \n",
       "2  a deadly attack on the police in egypt s weste...   \n",
       "3  north korea has backed presumptive u s republi...   \n",
       "4  north korea s latest missile launch undermines...   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0  thailand kick sumptuou funer king bhumibol adu...   \n",
       "1         california sue trump administr border wall   \n",
       "2  egypt western desert attack expos front outsid...   \n",
       "3  north korea say trump isnt screwi wise choic p...   \n",
       "4  nato say north korea missil launch undermin in...   \n",
       "\n",
       "                                           body_stem process_state  class  \n",
       "0  thailand wednesday mark start lavish five day ...          Pass      0  \n",
       "1  california file lawsuit wednesday trump admini...          Pass      0  \n",
       "2  deadli attack polic egypt western desert claim...          Pass      0  \n",
       "3  north korea back presumpt u republican nomine ...          Pass      0  \n",
       "4  north korea latest missil launch undermin regi...          Pass      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = files[0]\n",
    "loc = f'{folder_loc}{file}'\n",
    "print(loc)\n",
    "df_0, df_0_base_id_list = CM.get_df(loc,row_cnt,file,isPass)\n",
    "print(len(df_0))\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Raw/Fake.csv\n",
      "9999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title_url_min</th>\n",
       "      <th>body_url_min</th>\n",
       "      <th>title_url</th>\n",
       "      <th>body_url</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>process_state</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake_22077</td>\n",
       "      <td>HAWK OR NOT? Is Trump Expanding the Wars?</td>\n",
       "      <td>So once again, here we are. A new President mu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hawk or not? is trump expanding the wars?</td>\n",
       "      <td>so once again, here we are. a new president mu...</td>\n",
       "      <td>hawk or not is trump expanding the wars</td>\n",
       "      <td>so once again here we are a new president must...</td>\n",
       "      <td>hawk trump expand war</td>\n",
       "      <td>new presid must deal old war presid donald tru...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fake_4551</td>\n",
       "      <td>This Sickening Pro-Gun Meme Is Quite Possibly...</td>\n",
       "      <td>The pro-gun crowd loves guns first, and respec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this sickening pro-gun meme is quite possibly...</td>\n",
       "      <td>the pro-gun crowd loves guns first, and respec...</td>\n",
       "      <td>this sickening pro gun meme is quite possibly ...</td>\n",
       "      <td>the pro gun crowd loves guns first and respect...</td>\n",
       "      <td>sicken pro gun meme quit possibl vile thing in...</td>\n",
       "      <td>pro gun crowd love gun first respect last leas...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fake_22637</td>\n",
       "      <td>‘Vote All You Want, The Secret Government Won’...</td>\n",
       "      <td>21st Century Wire says Those who pull the stri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‘vote all you want, the secret government won’...</td>\n",
       "      <td>21st century wire says those who pull the stri...</td>\n",
       "      <td>vote all you want the secret government won t ...</td>\n",
       "      <td>st century wire says those who pull the string...</td>\n",
       "      <td>vote want secret govern chang</td>\n",
       "      <td>st centuri wire say pull string behind curtain...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fake_12265</td>\n",
       "      <td>BREAKING: MITT ROMNEY Speaks To Reporters Foll...</td>\n",
       "      <td>https://youtu.be/Ai5ayloRa-0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>breaking: mitt romney speaks to reporters foll...</td>\n",
       "      <td>https://youtu.be/ai5aylora-0</td>\n",
       "      <td>breaking mitt romney speaks to reporters follo...</td>\n",
       "      <td>https youtu be ai aylora</td>\n",
       "      <td>break mitt romney speak report follow dinner t...</td>\n",
       "      <td>http youtu ai aylora</td>\n",
       "      <td>Pass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fake_12968</td>\n",
       "      <td>WHOA! NEW DISTURBING VIDEO Shows HILLARY’S Cam...</td>\n",
       "      <td>On September 15, Hillary  apparently  held a r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>whoa! new disturbing video shows hillary’s cam...</td>\n",
       "      <td>on september 15, hillary apparently held a ral...</td>\n",
       "      <td>whoa new disturbing video shows hillary s camp...</td>\n",
       "      <td>on september hillary apparently held a rally i...</td>\n",
       "      <td>whoa new disturb video show hillari campaign l...</td>\n",
       "      <td>septemb hillari appar held ralli old student r...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  Fake_22077          HAWK OR NOT? Is Trump Expanding the Wars?   \n",
       "1   Fake_4551   This Sickening Pro-Gun Meme Is Quite Possibly...   \n",
       "2  Fake_22637  ‘Vote All You Want, The Secret Government Won’...   \n",
       "3  Fake_12265  BREAKING: MITT ROMNEY Speaks To Reporters Foll...   \n",
       "4  Fake_12968  WHOA! NEW DISTURBING VIDEO Shows HILLARY’S Cam...   \n",
       "\n",
       "                                                body  news_url  \\\n",
       "0  So once again, here we are. A new President mu...       NaN   \n",
       "1  The pro-gun crowd loves guns first, and respec...       NaN   \n",
       "2  21st Century Wire says Those who pull the stri...       NaN   \n",
       "3                       https://youtu.be/Ai5ayloRa-0       NaN   \n",
       "4  On September 15, Hillary  apparently  held a r...       NaN   \n",
       "\n",
       "                                       title_url_min  \\\n",
       "0          hawk or not? is trump expanding the wars?   \n",
       "1   this sickening pro-gun meme is quite possibly...   \n",
       "2  ‘vote all you want, the secret government won’...   \n",
       "3  breaking: mitt romney speaks to reporters foll...   \n",
       "4  whoa! new disturbing video shows hillary’s cam...   \n",
       "\n",
       "                                        body_url_min  \\\n",
       "0  so once again, here we are. a new president mu...   \n",
       "1  the pro-gun crowd loves guns first, and respec...   \n",
       "2  21st century wire says those who pull the stri...   \n",
       "3                       https://youtu.be/ai5aylora-0   \n",
       "4  on september 15, hillary apparently held a ral...   \n",
       "\n",
       "                                           title_url  \\\n",
       "0            hawk or not is trump expanding the wars   \n",
       "1  this sickening pro gun meme is quite possibly ...   \n",
       "2  vote all you want the secret government won t ...   \n",
       "3  breaking mitt romney speaks to reporters follo...   \n",
       "4  whoa new disturbing video shows hillary s camp...   \n",
       "\n",
       "                                            body_url  \\\n",
       "0  so once again here we are a new president must...   \n",
       "1  the pro gun crowd loves guns first and respect...   \n",
       "2  st century wire says those who pull the string...   \n",
       "3                           https youtu be ai aylora   \n",
       "4  on september hillary apparently held a rally i...   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0                              hawk trump expand war   \n",
       "1  sicken pro gun meme quit possibl vile thing in...   \n",
       "2                      vote want secret govern chang   \n",
       "3  break mitt romney speak report follow dinner t...   \n",
       "4  whoa new disturb video show hillari campaign l...   \n",
       "\n",
       "                                           body_stem process_state  class  \n",
       "0  new presid must deal old war presid donald tru...          Pass      1  \n",
       "1  pro gun crowd love gun first respect last leas...          Pass      1  \n",
       "2  st centuri wire say pull string behind curtain...          Pass      1  \n",
       "3                               http youtu ai aylora          Pass      1  \n",
       "4  septemb hillari appar held ralli old student r...          Pass      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = files[1]\n",
    "loc = f'{folder_loc}{file}'\n",
    "print(loc)\n",
    "df_1, df_1_base_id_list = CM.get_df(loc,row_cnt,file,isPass)\n",
    "print(len(df_1))\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Raw/URL_True.csv\n",
      "2045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>class</th>\n",
       "      <th>title_url_min</th>\n",
       "      <th>body_url_min</th>\n",
       "      <th>title_url</th>\n",
       "      <th>body_url</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>process_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True_kag_1673</td>\n",
       "      <td>https://www.reuters.com/article/us-nobel-prize...</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>'Who's Kazuo Ishiguro?' Japan asks, but celebr...</td>\n",
       "      <td>Author Kazuo Ishiguro speaks to the media outs...</td>\n",
       "      <td>0</td>\n",
       "      <td>'who's kazuo ishiguro?' japan asks, but celebr...</td>\n",
       "      <td>minutes after japanese-born briton kazuo ishi...</td>\n",
       "      <td>who s kazuo ishiguro japan asks but celebrates...</td>\n",
       "      <td>minutes after japanese born briton kazuo ishig...</td>\n",
       "      <td>who kazuo ishiguro japan ask celebr nobel author</td>\n",
       "      <td>minut japanes born briton kazuo ishiguro annou...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True_kag_3957</td>\n",
       "      <td>https://www.reuters.com/article/us-mexico-tele...</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>Mexican telecom regulator asks America Movil t...</td>\n",
       "      <td>FILE PHOTO - The logo of America Movil is pict...</td>\n",
       "      <td>0</td>\n",
       "      <td>mexican telecom regulator asks america movil t...</td>\n",
       "      <td>file photo - the logo of america movil is pict...</td>\n",
       "      <td>mexican telecom regulator asks america movil t...</td>\n",
       "      <td>file photo the logo of america movil is pictur...</td>\n",
       "      <td>mexican telecom regul ask america movil modifi...</td>\n",
       "      <td>file photo logo america movil pictur wall rece...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True_kag_1809</td>\n",
       "      <td>https://www.nytimes.com/2017/10/10/upshot/doub...</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>Doubtful Science Behind Arguments to Restrict ...</td>\n",
       "      <td>The ending date of 2002, even though we have m...</td>\n",
       "      <td>0</td>\n",
       "      <td>doubtful science behind arguments to restrict ...</td>\n",
       "      <td>in 2014, researchers published results from th...</td>\n",
       "      <td>doubtful science behind arguments to restrict ...</td>\n",
       "      <td>in researchers published results from the cont...</td>\n",
       "      <td>doubt scienc behind argument restrict birth co...</td>\n",
       "      <td>research publish result contracept choic proje...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True_kag_2068</td>\n",
       "      <td>http://inhealth.cnn.com/your-guide-to-treating...</td>\n",
       "      <td>inhealth.cnn.com</td>\n",
       "      <td>The Progression of Type 2 Diabetes Treatments</td>\n",
       "      <td>The Progression of Type 2 Diabetes Treatments ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the progression of type 2 diabetes treatments</td>\n",
       "      <td>getty each year, about 1.4 million people are ...</td>\n",
       "      <td>the progression of type diabetes treatments</td>\n",
       "      <td>getty each year about million people are diagn...</td>\n",
       "      <td>progress type diabet treatment</td>\n",
       "      <td>getti year million peopl diagnos type diabet m...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True_kag_904</td>\n",
       "      <td>http://www.bbc.com/news/world-asia-china-41564...</td>\n",
       "      <td>bbc.com</td>\n",
       "      <td>China's secret aid empire uncovered</td>\n",
       "      <td>Image copyright Paula Bronstein\\nChina has a l...</td>\n",
       "      <td>0</td>\n",
       "      <td>china's secret aid empire uncovered</td>\n",
       "      <td>the team's other major finding: when china giv...</td>\n",
       "      <td>china s secret aid empire uncovered</td>\n",
       "      <td>the team s other major finding when china give...</td>\n",
       "      <td>china secret aid empir uncov</td>\n",
       "      <td>team major find china give tradit aid recipi c...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           news_url  \\\n",
       "0  True_kag_1673  https://www.reuters.com/article/us-nobel-prize...   \n",
       "1  True_kag_3957  https://www.reuters.com/article/us-mexico-tele...   \n",
       "2  True_kag_1809  https://www.nytimes.com/2017/10/10/upshot/doub...   \n",
       "3  True_kag_2068  http://inhealth.cnn.com/your-guide-to-treating...   \n",
       "4   True_kag_904  http://www.bbc.com/news/world-asia-china-41564...   \n",
       "\n",
       "             source                                              title  \\\n",
       "0       reuters.com  'Who's Kazuo Ishiguro?' Japan asks, but celebr...   \n",
       "1       reuters.com  Mexican telecom regulator asks America Movil t...   \n",
       "2       nytimes.com  Doubtful Science Behind Arguments to Restrict ...   \n",
       "3  inhealth.cnn.com      The Progression of Type 2 Diabetes Treatments   \n",
       "4           bbc.com                China's secret aid empire uncovered   \n",
       "\n",
       "                                                body  class  \\\n",
       "0  Author Kazuo Ishiguro speaks to the media outs...      0   \n",
       "1  FILE PHOTO - The logo of America Movil is pict...      0   \n",
       "2  The ending date of 2002, even though we have m...      0   \n",
       "3  The Progression of Type 2 Diabetes Treatments ...      0   \n",
       "4  Image copyright Paula Bronstein\\nChina has a l...      0   \n",
       "\n",
       "                                       title_url_min  \\\n",
       "0  'who's kazuo ishiguro?' japan asks, but celebr...   \n",
       "1  mexican telecom regulator asks america movil t...   \n",
       "2  doubtful science behind arguments to restrict ...   \n",
       "3      the progression of type 2 diabetes treatments   \n",
       "4                china's secret aid empire uncovered   \n",
       "\n",
       "                                        body_url_min  \\\n",
       "0   minutes after japanese-born briton kazuo ishi...   \n",
       "1  file photo - the logo of america movil is pict...   \n",
       "2  in 2014, researchers published results from th...   \n",
       "3  getty each year, about 1.4 million people are ...   \n",
       "4  the team's other major finding: when china giv...   \n",
       "\n",
       "                                           title_url  \\\n",
       "0  who s kazuo ishiguro japan asks but celebrates...   \n",
       "1  mexican telecom regulator asks america movil t...   \n",
       "2  doubtful science behind arguments to restrict ...   \n",
       "3        the progression of type diabetes treatments   \n",
       "4                china s secret aid empire uncovered   \n",
       "\n",
       "                                            body_url  \\\n",
       "0  minutes after japanese born briton kazuo ishig...   \n",
       "1  file photo the logo of america movil is pictur...   \n",
       "2  in researchers published results from the cont...   \n",
       "3  getty each year about million people are diagn...   \n",
       "4  the team s other major finding when china give...   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0   who kazuo ishiguro japan ask celebr nobel author   \n",
       "1  mexican telecom regul ask america movil modifi...   \n",
       "2  doubt scienc behind argument restrict birth co...   \n",
       "3                     progress type diabet treatment   \n",
       "4                       china secret aid empir uncov   \n",
       "\n",
       "                                           body_stem process_state  \n",
       "0  minut japanes born briton kazuo ishiguro annou...          Pass  \n",
       "1  file photo logo america movil pictur wall rece...          Pass  \n",
       "2  research publish result contracept choic proje...          Pass  \n",
       "3  getti year million peopl diagnos type diabet m...          Pass  \n",
       "4  team major find china give tradit aid recipi c...          Pass  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = files[2]\n",
    "loc = f'{folder_loc}{file}'\n",
    "print(loc)\n",
    "df_2, df_2_base_id_list = CM.get_df(loc,row_cnt,file,isPass)\n",
    "print(len(df_2))\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Raw/URL_Fake.csv\n",
      "1078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>class</th>\n",
       "      <th>title_url_min</th>\n",
       "      <th>body_url_min</th>\n",
       "      <th>title_url</th>\n",
       "      <th>body_url</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>process_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake_kag_3002</td>\n",
       "      <td>http://beforeitsnews.com/u-s-politics/2017/09/...</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>San Juan Mayor Sports ‘Help Us We Are Dying’ T...</td>\n",
       "      <td>San Juan Mayor Sports ‘Help Us We Are Dying’ T...</td>\n",
       "      <td>1</td>\n",
       "      <td>san juan mayor sports ‘help us we are dying’ t...</td>\n",
       "      <td>san juan mayor sports ‘help us we are dying’ t...</td>\n",
       "      <td>san juan mayor sports help us we are dying t s...</td>\n",
       "      <td>san juan mayor sports help us we are dying t s...</td>\n",
       "      <td>san juan mayor sport help us die shirt anti tr...</td>\n",
       "      <td>san juan mayor sport help us die shirt anti tr...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fake_kag_3697</td>\n",
       "      <td>https://www.activistpost.com/2017/10/another-m...</td>\n",
       "      <td>activistpost.com</td>\n",
       "      <td>Another Mass Shooting, Another Grab For Guns: ...</td>\n",
       "      <td>By Tony Cartalucci\\n \\nNothing is more deplora...</td>\n",
       "      <td>1</td>\n",
       "      <td>another mass shooting, another grab for guns: ...</td>\n",
       "      <td>by tony cartalucci nothing is more deplorable ...</td>\n",
       "      <td>another mass shooting another grab for guns gu...</td>\n",
       "      <td>by tony cartalucci nothing is more deplorable ...</td>\n",
       "      <td>anoth mass shoot anoth grab gun gun fact</td>\n",
       "      <td>toni cartalucci noth deplor hijack human trage...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fake_pol_190</td>\n",
       "      <td>https://www.rollingstone.com/music/news/miley-...</td>\n",
       "      <td>rollingstone.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>miley cyrus: ‘donald trump is a f–king nightmare’</td>\n",
       "      <td>donald trump took a major step toward securing...</td>\n",
       "      <td>miley cyrus donald trump is a f king nightmare</td>\n",
       "      <td>donald trump took a major step toward securing...</td>\n",
       "      <td>miley cyru donald trump f king nightmar</td>\n",
       "      <td>donald trump took major step toward secur repu...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fake_kag_3898</td>\n",
       "      <td>http://beforeitsnews.com/sports/2017/10/crossf...</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>Crossfit Training: The Upperclassman</td>\n",
       "      <td>Crossfit Training: The Upperclassman\\n(Before ...</td>\n",
       "      <td>1</td>\n",
       "      <td>crossfit training: the upperclassman</td>\n",
       "      <td>you’ve been completing wods consistently, you...</td>\n",
       "      <td>crossfit training the upperclassman</td>\n",
       "      <td>you ve been completing wods consistently you r...</td>\n",
       "      <td>crossfit train upperclassman</td>\n",
       "      <td>complet wod consist gradual improv time contro...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fake_pol_226</td>\n",
       "      <td>https://web.archive.org/web/20171101100601/htt...</td>\n",
       "      <td>snapusanews.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>morgan freeman: ‘jailing hillary’ best way to ...</td>\n",
       "      <td>302 shares share tweet the best way to restore...</td>\n",
       "      <td>morgan freeman jailing hillary best way to res...</td>\n",
       "      <td>shares share tweet the best way to restore pub...</td>\n",
       "      <td>morgan freeman jail hillari best way restor pu...</td>\n",
       "      <td>share share tweet best way restor public faith...</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           news_url  \\\n",
       "0  Fake_kag_3002  http://beforeitsnews.com/u-s-politics/2017/09/...   \n",
       "1  Fake_kag_3697  https://www.activistpost.com/2017/10/another-m...   \n",
       "2   Fake_pol_190  https://www.rollingstone.com/music/news/miley-...   \n",
       "3  Fake_kag_3898  http://beforeitsnews.com/sports/2017/10/crossf...   \n",
       "4   Fake_pol_226  https://web.archive.org/web/20171101100601/htt...   \n",
       "\n",
       "              source                                              title  \\\n",
       "0  beforeitsnews.com  San Juan Mayor Sports ‘Help Us We Are Dying’ T...   \n",
       "1   activistpost.com  Another Mass Shooting, Another Grab For Guns: ...   \n",
       "2   rollingstone.com                                                NaN   \n",
       "3  beforeitsnews.com               Crossfit Training: The Upperclassman   \n",
       "4    snapusanews.com                                                NaN   \n",
       "\n",
       "                                                body  class  \\\n",
       "0  San Juan Mayor Sports ‘Help Us We Are Dying’ T...      1   \n",
       "1  By Tony Cartalucci\\n \\nNothing is more deplora...      1   \n",
       "2                                                NaN      1   \n",
       "3  Crossfit Training: The Upperclassman\\n(Before ...      1   \n",
       "4                                                NaN      1   \n",
       "\n",
       "                                       title_url_min  \\\n",
       "0  san juan mayor sports ‘help us we are dying’ t...   \n",
       "1  another mass shooting, another grab for guns: ...   \n",
       "2  miley cyrus: ‘donald trump is a f–king nightmare’   \n",
       "3               crossfit training: the upperclassman   \n",
       "4  morgan freeman: ‘jailing hillary’ best way to ...   \n",
       "\n",
       "                                        body_url_min  \\\n",
       "0  san juan mayor sports ‘help us we are dying’ t...   \n",
       "1  by tony cartalucci nothing is more deplorable ...   \n",
       "2  donald trump took a major step toward securing...   \n",
       "3   you’ve been completing wods consistently, you...   \n",
       "4  302 shares share tweet the best way to restore...   \n",
       "\n",
       "                                           title_url  \\\n",
       "0  san juan mayor sports help us we are dying t s...   \n",
       "1  another mass shooting another grab for guns gu...   \n",
       "2     miley cyrus donald trump is a f king nightmare   \n",
       "3                crossfit training the upperclassman   \n",
       "4  morgan freeman jailing hillary best way to res...   \n",
       "\n",
       "                                            body_url  \\\n",
       "0  san juan mayor sports help us we are dying t s...   \n",
       "1  by tony cartalucci nothing is more deplorable ...   \n",
       "2  donald trump took a major step toward securing...   \n",
       "3  you ve been completing wods consistently you r...   \n",
       "4  shares share tweet the best way to restore pub...   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0  san juan mayor sport help us die shirt anti tr...   \n",
       "1           anoth mass shoot anoth grab gun gun fact   \n",
       "2            miley cyru donald trump f king nightmar   \n",
       "3                       crossfit train upperclassman   \n",
       "4  morgan freeman jail hillari best way restor pu...   \n",
       "\n",
       "                                           body_stem process_state  \n",
       "0  san juan mayor sport help us die shirt anti tr...          Pass  \n",
       "1  toni cartalucci noth deplor hijack human trage...          Pass  \n",
       "2  donald trump took major step toward secur repu...          Pass  \n",
       "3  complet wod consist gradual improv time contro...          Pass  \n",
       "4  share share tweet best way restor public faith...          Pass  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = files[3]\n",
    "loc = f'{folder_loc}{file}'\n",
    "print(loc)\n",
    "df_3, df_3_base_id_list = CM.get_df(loc,row_cnt,file,isPass)\n",
    "print(len(df_3))\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_len = min(len(df_0),len(df_1))\n",
    "# df_0 = df_0.sample(n=min_len,random_state=42)\n",
    "# print(len(df_0))\n",
    "# df_1 = df_1.sample(n=min_len,random_state=42)\n",
    "# print(len(df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a= df_0.append(df_1,ignore_index=True)\n",
    "df_a = df_a.sample(n=len(df_a))\n",
    "df_a.reset_index(drop=True,inplace=True)\n",
    "df_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_len = min(len(df_2),len(df_3))\n",
    "# df_2 = df_2.sample(n=min_len,random_state=42)\n",
    "# print(len(df_2))\n",
    "# df_3 = df_3.sample(n=min_len,random_state=42)\n",
    "# print(len(df_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3123, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b= df_2.append(df_3,ignore_index=True)\n",
    "df_b = df_b.sample(n=len(df_b))\n",
    "df_b.reset_index(drop=True,inplace=True)\n",
    "df_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23122, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df_a.append(df_b,ignore_index=True)\n",
    "df = df.sample(n=len(df))\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n = 1000,random_state=42)\n",
    "# df.reset_index(drop=True,inplace=True)\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pick 80% of records\n",
    "# train_idx, test_idx, y_train, y_test = train_test_split(df.id, df['class'], test_size=0.2, stratify=df['class'], random_state=42)\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(section_sample, y_sample, test_size=0.2, stratify=y_sample, random_state=42)\n",
    "# print(len(train_idx),len(test_idx))\n",
    "# print(train_idx.tolist()[:10])\n",
    "# type(train_idx.tolist())\n",
    "# train_idx.to_csv(f'{save_loc}train_idx.csv',mode='w', header=True, index=False)\n",
    "# test_idx.to_csv(f'{save_loc}test_idx.csv',mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_len = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/TF_dicts/\n",
      "dataset/Raw/LIWC/Stemmed_All/\n",
      "Build url Dict, in loc: dataset/Raw/LIWC/Stemmed_All/\n",
      "0 Fake_4551\n",
      "250 True_16528\n",
      "500 True_15874\n",
      "750 Fake_13233\n",
      "1000 True_5353\n",
      "1250 Fake_12202\n",
      "1500 Fake_22316\n",
      "1750 Fake_7511\n",
      "2000 Fake_20701\n",
      "2250 Fake_4245\n",
      "2500 Fake_8832\n",
      "2750 True_10396\n",
      "3000 Fake_4177\n",
      "3250 True_20641\n",
      "3500 True_kag_2018\n",
      "3750 True_11197\n",
      "4000 True_kag_1911\n",
      "4250 True_909\n",
      "4500 True_12286\n",
      "4750 Fake_16121\n",
      "5000 Fake_20151\n",
      "5250 Fake_19533\n",
      "5500 True_1846\n",
      "5750 True_10118\n",
      "6000 True_7468\n",
      "6250 True_17809\n",
      "6500 True_kag_1749\n",
      "6750 Fake_6814\n",
      "7000 Fake_pol_158\n",
      "7250 Fake_1835\n",
      "7500 Fake_18603\n",
      "7750 True_11514\n",
      "8000 True_8981\n",
      "8250 Fake_19336\n",
      "8500 True_11230\n",
      "8750 True_1001\n",
      "9000 Fake_8713\n",
      "9250 Fake_13606\n",
      "9500 Fake_18736\n",
      "9750 True_2064\n",
      "10000 Fake_14047\n",
      "10250 Fake_11248\n",
      "10500 True_kag_1794\n",
      "10750 Fake_10518\n",
      "11000 True_8422\n",
      "11250 Fake_8609\n",
      "11500 True_kag_2834\n",
      "11750 True_20317\n",
      "12000 True_12900\n",
      "12250 True_kag_762\n",
      "12500 Fake_14821\n",
      "12750 Fake_2729\n",
      "13000 True_2810\n",
      "13250 Fake_15908\n",
      "13500 Fake_kag_2082\n",
      "13750 Fake_11891\n",
      "14000 True_20304\n",
      "14250 True_15311\n",
      "14500 True_13336\n",
      "14750 True_17458\n",
      "15000 True_947\n",
      "15250 True_12342\n",
      "15500 Fake_13792\n",
      "15750 Fake_9381\n",
      "16000 True_18996\n",
      "16250 True_20369\n",
      "16500 True_110\n",
      "16750 True_kag_3203\n",
      "17000 True_17655\n",
      "17250 True_16975\n",
      "17500 True_kag_3411\n",
      "17750 Fake_13071\n",
      "18000 True_pol_27\n",
      "18250 Fake_2734\n",
      "18500 Fake_22878\n",
      "18750 True_kag_2645\n",
      "19000 True_8284\n",
      "19250 Fake_18113\n",
      "19500 True_17151\n",
      "19750 True_2703\n",
      "20000 True_14662\n",
      "20250 Fake_kag_834\n",
      "20500 Fake_23472\n",
      "20750 Fake_5406\n",
      "21000 True_3854\n",
      "21250 True_kag_2150\n",
      "21500 Fake_7264\n",
      "21750 Fake_18853\n",
      "22000 Fake_6958\n",
      "22250 True_706\n",
      "22500 Fake_19046\n",
      "22750 True_18864\n",
      "23000 Fake_10924\n",
      "Length of Body dict:  95988\n",
      "Length of Title dict:  17324\n",
      "Done\n",
      "Build stem Dict, in loc: dataset/Raw/LIWC/Stemmed_All/\n",
      "0 Fake_4551\n",
      "250 True_16528\n",
      "500 True_15874\n",
      "750 Fake_13233\n",
      "1000 True_5353\n",
      "1250 Fake_12202\n",
      "1500 Fake_22316\n",
      "1750 Fake_7511\n",
      "2000 Fake_20701\n",
      "2250 Fake_4245\n",
      "2500 Fake_8832\n",
      "2750 True_10396\n",
      "3000 Fake_4177\n",
      "3250 True_20641\n",
      "3500 True_kag_2018\n",
      "3750 True_11197\n",
      "4000 True_kag_1911\n",
      "4250 True_909\n",
      "4500 True_12286\n",
      "4750 Fake_16121\n",
      "5000 Fake_20151\n",
      "5250 Fake_19533\n",
      "5500 True_1846\n",
      "5750 True_10118\n",
      "6000 True_7468\n",
      "6250 True_17809\n",
      "6500 True_kag_1749\n",
      "6750 Fake_6814\n",
      "7000 Fake_pol_158\n",
      "7250 Fake_1835\n",
      "7500 Fake_18603\n",
      "7750 True_11514\n",
      "8000 True_8981\n",
      "8250 Fake_19336\n",
      "8500 True_11230\n",
      "8750 True_1001\n",
      "9000 Fake_8713\n",
      "9250 Fake_13606\n",
      "9500 Fake_18736\n",
      "9750 True_2064\n",
      "10000 Fake_14047\n",
      "10250 Fake_11248\n",
      "10500 True_kag_1794\n",
      "10750 Fake_10518\n",
      "11000 True_8422\n",
      "11250 Fake_8609\n",
      "11500 True_kag_2834\n",
      "11750 True_20317\n",
      "12000 True_12900\n",
      "12250 True_kag_762\n",
      "12500 Fake_14821\n",
      "12750 Fake_2729\n",
      "13000 True_2810\n",
      "13250 Fake_15908\n",
      "13500 Fake_kag_2082\n",
      "13750 Fake_11891\n",
      "14000 True_20304\n",
      "14250 True_15311\n",
      "14500 True_13336\n",
      "14750 True_17458\n",
      "15000 True_947\n",
      "15250 True_12342\n",
      "15500 Fake_13792\n",
      "15750 Fake_9381\n",
      "16000 True_18996\n",
      "16250 True_20369\n",
      "16500 True_110\n",
      "16750 True_kag_3203\n",
      "17000 True_17655\n",
      "17250 True_16975\n",
      "17500 True_kag_3411\n",
      "17750 Fake_13071\n",
      "18000 True_pol_27\n",
      "18250 Fake_2734\n",
      "18500 Fake_22878\n",
      "18750 True_kag_2645\n",
      "19000 True_8284\n",
      "19250 Fake_18113\n",
      "19500 True_17151\n",
      "19750 True_2703\n",
      "20000 True_14662\n",
      "20250 Fake_kag_834\n",
      "20500 Fake_23472\n",
      "20750 Fake_5406\n",
      "21000 True_3854\n",
      "21250 True_kag_2150\n",
      "21500 Fake_7264\n",
      "21750 Fake_18853\n",
      "22000 Fake_6958\n",
      "22250 True_706\n",
      "22500 Fake_19046\n",
      "22750 True_18864\n",
      "23000 Fake_10924\n",
      "Length of Body dict:  72556\n",
      "Length of Title dict:  11692\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Delete all TF_dicts:\n",
    "loc = f'{save_loc}TF_dicts/'\n",
    "print('Delete loc: ',loc)\n",
    "for f in os.listdir(loc):\n",
    "    os.remove(os.path.join(loc, f))\n",
    "\n",
    "print(save_loc)\n",
    "for col in ['url','stem']:\n",
    "    gc.collect()\n",
    "    print(f'Build {col} Dict, in loc: {save_loc}')\n",
    "    bodyDict = {}\n",
    "    titleDict = {}\n",
    "    stop = stopwords.words('english')\n",
    "    for idx,row in df.iterrows():\n",
    "        row_body_dict = {}\n",
    "        row_title_dict = {}\n",
    "        # print(row.id)\n",
    "            \n",
    "        if idx%cycle_len == 0 :\n",
    "            print(idx,row.id)\n",
    "        try:\n",
    "            body_text = CM.non_stem(row[f'body_{col}'])\n",
    "            body_len = len(body_text.split())\n",
    "            for word in body_text.split():\n",
    "                word = str(word)\n",
    "\n",
    "                if ((word not in stop) and (word not in row_body_dict.keys())):\n",
    "                    row_body_dict[word] = 1\n",
    "                    if word not in bodyDict.keys():\n",
    "                        bodyDict[word] = 1\n",
    "                elif word not in stop:\n",
    "                    row_body_dict[word] += 1      \n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            title_text = CM.non_stem(row[f'title_{col}'])\n",
    "            title_len = len(title_text.split())\n",
    "            for word in title_text.split():\n",
    "                word = str(word)\n",
    "                if (( word not in stop) and (word not in row_title_dict.keys())):\n",
    "                    row_title_dict[word] = 1\n",
    "                    if ((word not in stop) and (word not in titleDict.keys())):\n",
    "                        titleDict[word] = 1\n",
    "                elif word not in stop:\n",
    "                    row_title_dict[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "                            \n",
    "        for key in row_body_dict.keys():\n",
    "            row_body_dict[key] = row_body_dict[key]/float(body_len)\n",
    "        for key in row_title_dict.keys():\n",
    "            row_title_dict[key] = row_title_dict[key]/float(title_len)\n",
    "        \n",
    "        if((len(row_body_dict.keys())>1) and len(row_title_dict.keys())>1):\n",
    "            # Save row dicts\n",
    "            loc= f'{save_loc}TF_dicts/bodyDict_{col}_{row.id}.csv'\n",
    "            CM.save_dict(loc,row_body_dict)\n",
    "\n",
    "            loc= f'{save_loc}TF_dicts/titleDict_{col}_{row.id}.csv'\n",
    "            CM.save_dict(loc,row_title_dict)\n",
    "\n",
    "            \n",
    "    print('Length of Body dict: ',len(list(bodyDict.keys())))\n",
    "    print('Length of Title dict: ',len(list(titleDict.keys())))   \n",
    "    print('Done')\n",
    "    \n",
    "    \n",
    "    # Save master dicts\n",
    "    loc= f'{save_loc}Master_dicts/bodyDict_{col}.csv'\n",
    "    CM.save_dict(loc,bodyDict)\n",
    "\n",
    "    loc= f'{save_loc}Master_dicts/titleDict_{col}.csv'\n",
    "    CM.save_dict(loc,titleDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body url\n",
      "Total records: 22946\n",
      "0: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_0.csv\n",
      "500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_11060.csv\n",
      "1000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_12278.csv\n",
      "1500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_13439.csv\n",
      "2000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_14442.csv\n",
      "2500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_15481.csv\n",
      "3000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_16577.csv\n",
      "3500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_176.csv\n",
      "4000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_1857.csv\n",
      "4500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_1970.csv\n",
      "5000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_20747.csv\n",
      "5500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_21824.csv\n",
      "6000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_22893.csv\n",
      "6500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_2815.csv\n",
      "7000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_3801.csv\n",
      "7500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_4833.csv\n",
      "8000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_5827.csv\n",
      "8500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_686.csv\n",
      "9000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_7875.csv\n",
      "9500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_8935.csv\n",
      "10000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_kag_1044.csv\n",
      "10500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_Fake_kag_3561.csv\n",
      "11000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_10030.csv\n",
      "11500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_11021.csv\n",
      "12000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_1201.csv\n",
      "12500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_12977.csv\n",
      "13000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_13925.csv\n",
      "13500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_14911.csv\n",
      "14000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_15852.csv\n",
      "14500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_16824.csv\n",
      "15000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_17729.csv\n",
      "15500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_18665.csv\n",
      "16000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_19595.csv\n",
      "16500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_20585.csv\n",
      "17000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_2280.csv\n",
      "17500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_3221.csv\n",
      "18000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_4202.csv\n",
      "18500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_5128.csv\n",
      "19000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_6059.csv\n",
      "19500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_706.csv\n",
      "20000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_8051.csv\n",
      "20500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_9075.csv\n",
      "21000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_kag_1032.csv\n",
      "21500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_kag_2169.csv\n",
      "22000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_kag_3318.csv\n",
      "22500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_url_True_kag_886.csv\n",
      "Length of body_urldict: 95987\n",
      "title url\n",
      "Total records: 22946\n",
      "0: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_0.csv\n",
      "500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_11060.csv\n",
      "1000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_12278.csv\n",
      "1500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_13439.csv\n",
      "2000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_14442.csv\n",
      "2500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_15481.csv\n",
      "3000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_16577.csv\n",
      "3500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_176.csv\n",
      "4000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_1857.csv\n",
      "4500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_1970.csv\n",
      "5000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_20747.csv\n",
      "5500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_21824.csv\n",
      "6000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_22893.csv\n",
      "6500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_2815.csv\n",
      "7000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_3801.csv\n",
      "7500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_4833.csv\n",
      "8000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_5827.csv\n",
      "8500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_686.csv\n",
      "9000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_7875.csv\n",
      "9500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_8935.csv\n",
      "10000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_kag_1044.csv\n",
      "10500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_Fake_kag_3561.csv\n",
      "11000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_10030.csv\n",
      "11500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_11021.csv\n",
      "12000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_1201.csv\n",
      "12500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_12977.csv\n",
      "13000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_13925.csv\n",
      "13500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_14911.csv\n",
      "14000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_15852.csv\n",
      "14500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_16824.csv\n",
      "15000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_17729.csv\n",
      "15500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_18665.csv\n",
      "16000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_19595.csv\n",
      "16500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_20585.csv\n",
      "17000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_2280.csv\n",
      "17500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_3221.csv\n",
      "18000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_4202.csv\n",
      "18500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_5128.csv\n",
      "19000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_6059.csv\n",
      "19500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_706.csv\n",
      "20000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_8051.csv\n",
      "20500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_9075.csv\n",
      "21000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_kag_1032.csv\n",
      "21500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_kag_2169.csv\n",
      "22000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_kag_3318.csv\n",
      "22500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_url_True_kag_886.csv\n",
      "Length of title_urldict: 17323\n",
      "body stem\n",
      "Total records: 22946\n",
      "0: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_0.csv\n",
      "500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_11060.csv\n",
      "1000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_12278.csv\n",
      "1500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_13439.csv\n",
      "2000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_14442.csv\n",
      "2500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_15481.csv\n",
      "3000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_16577.csv\n",
      "3500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_176.csv\n",
      "4000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_1857.csv\n",
      "4500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_1970.csv\n",
      "5000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_20747.csv\n",
      "5500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_21824.csv\n",
      "6000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_22893.csv\n",
      "6500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_2815.csv\n",
      "7000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_3801.csv\n",
      "7500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_4833.csv\n",
      "8000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_5827.csv\n",
      "8500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_686.csv\n",
      "9000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_7875.csv\n",
      "9500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_8935.csv\n",
      "10000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_kag_1044.csv\n",
      "10500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_Fake_kag_3561.csv\n",
      "11000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_10030.csv\n",
      "11500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_11021.csv\n",
      "12000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_1201.csv\n",
      "12500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_12977.csv\n",
      "13000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_13925.csv\n",
      "13500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_14911.csv\n",
      "14000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_15852.csv\n",
      "14500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_16824.csv\n",
      "15000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_17729.csv\n",
      "15500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_18665.csv\n",
      "16000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_19595.csv\n",
      "16500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_20585.csv\n",
      "17000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_2280.csv\n",
      "17500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_3221.csv\n",
      "18000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_4202.csv\n",
      "18500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_5128.csv\n",
      "19000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_6059.csv\n",
      "19500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_706.csv\n",
      "20000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_8051.csv\n",
      "20500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_9075.csv\n",
      "21000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_kag_1032.csv\n",
      "21500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_kag_2169.csv\n",
      "22000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_kag_3318.csv\n",
      "22500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/bodyDict_stem_True_kag_886.csv\n",
      "Length of body_stemdict: 72555\n",
      "title stem\n",
      "Total records: 22946\n",
      "0: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_0.csv\n",
      "500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_11060.csv\n",
      "1000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_12278.csv\n",
      "1500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_13439.csv\n",
      "2000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_14442.csv\n",
      "2500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_15481.csv\n",
      "3000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_16577.csv\n",
      "3500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_176.csv\n",
      "4000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_1857.csv\n",
      "4500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_1970.csv\n",
      "5000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_20747.csv\n",
      "5500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_21824.csv\n",
      "6000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_22893.csv\n",
      "6500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_2815.csv\n",
      "7000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_3801.csv\n",
      "7500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_4833.csv\n",
      "8000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_5827.csv\n",
      "8500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_686.csv\n",
      "9000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_7875.csv\n",
      "9500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_8935.csv\n",
      "10000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_kag_1044.csv\n",
      "10500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_Fake_kag_3561.csv\n",
      "11000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_10030.csv\n",
      "11500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_11021.csv\n",
      "12000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_1201.csv\n",
      "12500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_12977.csv\n",
      "13000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_13925.csv\n",
      "13500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_14911.csv\n",
      "14000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_15852.csv\n",
      "14500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_16824.csv\n",
      "15000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_17729.csv\n",
      "15500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_18665.csv\n",
      "16000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_19595.csv\n",
      "16500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_20585.csv\n",
      "17000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_2280.csv\n",
      "17500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_3221.csv\n",
      "18000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_4202.csv\n",
      "18500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_5128.csv\n",
      "19000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_6059.csv\n",
      "19500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_706.csv\n",
      "20000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_8051.csv\n",
      "20500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_9075.csv\n",
      "21000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_kag_1032.csv\n",
      "21500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_kag_2169.csv\n",
      "22000: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_kag_3318.csv\n",
      "22500: dataset/Raw/LIWC/Stemmed_All/TF_dicts/titleDict_stem_True_kag_886.csv\n",
      "Length of title_stemdict: 11691\n"
     ]
    }
   ],
   "source": [
    "# Get IDF dict\n",
    "for col in ['url','stem']:\n",
    "    for part in ['body','title']:\n",
    "        print(part,col)\n",
    "        file_loc_list = [arg for arg in os.listdir(f'{save_loc}TF_dicts') if ((col in arg) and (part in arg) and ('User' not in arg))]\n",
    "        main_dict_name = f'{save_loc}Master_dicts/{part}Dict_{col}'\n",
    "        idfDict= CM.build_IDF(f'{save_loc}',file_loc_list, main_dict_name)\n",
    "\n",
    "        # Save idfDict\n",
    "        loc= f'{save_loc}IDFs/{part}_{col}.csv'\n",
    "        print(f'Length of {part}_{col}dict: {len(list(idfDict.keys()))}')\n",
    "        CM.save_dict(loc,idfDict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/Raw/LIWC/Stemmed_All/bodyDict_stem.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{save_loc}bodyDict_{col}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/URL/BOW/\n",
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/Stem/BOW/\n",
      "idx:  0\n",
      "idx:  250\n",
      "idx:  500\n",
      "idx:  750\n",
      "idx:  1000\n",
      "idx:  1250\n",
      "idx:  1500\n",
      "idx:  1750\n",
      "idx:  2000\n",
      "idx:  2250\n",
      "idx:  2500\n",
      "idx:  2750\n",
      "idx:  3000\n",
      "idx:  3250\n",
      "idx:  3500\n",
      "idx:  3750\n",
      "idx:  4000\n",
      "idx:  4250\n",
      "idx:  4500\n",
      "idx:  4750\n",
      "idx:  5000\n",
      "idx:  5250\n",
      "idx:  5500\n",
      "idx:  5750\n",
      "idx:  6000\n",
      "idx:  6250\n",
      "idx:  6500\n",
      "idx:  6750\n",
      "idx:  7000\n",
      "idx:  7250\n",
      "idx:  7500\n",
      "idx:  7750\n",
      "idx:  8000\n",
      "idx:  8250\n",
      "idx:  8500\n",
      "idx:  8750\n",
      "idx:  9000\n",
      "idx:  9250\n",
      "idx:  9500\n",
      "idx:  9750\n",
      "idx:  10000\n",
      "idx:  10250\n",
      "idx:  10500\n",
      "idx:  10750\n",
      "idx:  11000\n",
      "idx:  11250\n",
      "idx:  11500\n",
      "idx:  11750\n",
      "idx:  12000\n",
      "idx:  12250\n",
      "idx:  12500\n",
      "idx:  12750\n",
      "idx:  13000\n",
      "idx:  13250\n",
      "idx:  13500\n",
      "idx:  13750\n",
      "idx:  14000\n",
      "idx:  14250\n",
      "idx:  14500\n",
      "idx:  14750\n",
      "idx:  15000\n",
      "idx:  15250\n",
      "idx:  15500\n",
      "idx:  15750\n",
      "idx:  16000\n",
      "idx:  16250\n",
      "idx:  16500\n",
      "idx:  16750\n",
      "idx:  17000\n",
      "idx:  17250\n",
      "idx:  17500\n",
      "idx:  17750\n",
      "idx:  18000\n",
      "idx:  18250\n",
      "idx:  18500\n",
      "idx:  18750\n",
      "idx:  19000\n",
      "idx:  19250\n",
      "idx:  19500\n",
      "idx:  19750\n",
      "idx:  20000\n",
      "idx:  20250\n",
      "idx:  20500\n",
      "idx:  20750\n",
      "idx:  21000\n",
      "idx:  21250\n",
      "idx:  21500\n",
      "idx:  21750\n",
      "idx:  22000\n",
      "idx:  22250\n",
      "idx:  22500\n",
      "idx:  22750\n",
      "idx:  23000\n",
      "Percent of words found in dictionary 77.96%\n",
      "Next Cycle: ###################################################################\n",
      "idx:  0\n",
      "idx:  250\n",
      "idx:  500\n",
      "idx:  750\n",
      "idx:  1000\n",
      "idx:  1250\n",
      "idx:  1500\n",
      "idx:  1750\n",
      "idx:  2000\n",
      "idx:  2250\n",
      "idx:  2500\n",
      "idx:  2750\n",
      "idx:  3000\n",
      "idx:  3250\n",
      "idx:  3500\n",
      "idx:  3750\n",
      "idx:  4000\n",
      "idx:  4250\n",
      "idx:  4500\n",
      "idx:  4750\n",
      "idx:  5000\n",
      "idx:  5250\n",
      "idx:  5500\n",
      "idx:  5750\n",
      "idx:  6000\n",
      "idx:  6250\n",
      "idx:  6500\n",
      "idx:  6750\n",
      "idx:  7000\n",
      "idx:  7250\n",
      "idx:  7500\n",
      "idx:  7750\n",
      "idx:  8000\n",
      "idx:  8250\n",
      "idx:  8500\n",
      "idx:  8750\n",
      "idx:  9000\n",
      "idx:  9250\n",
      "idx:  9500\n",
      "idx:  9750\n",
      "idx:  10000\n",
      "idx:  10250\n",
      "idx:  10500\n",
      "idx:  10750\n",
      "idx:  11000\n",
      "idx:  11250\n",
      "idx:  11500\n",
      "idx:  11750\n",
      "idx:  12000\n",
      "idx:  12250\n",
      "idx:  12500\n",
      "idx:  12750\n",
      "idx:  13000\n",
      "idx:  13250\n",
      "idx:  13500\n",
      "idx:  13750\n",
      "idx:  14000\n",
      "idx:  14250\n",
      "idx:  14500\n",
      "idx:  14750\n",
      "idx:  15000\n",
      "idx:  15250\n",
      "idx:  15500\n",
      "idx:  15750\n",
      "idx:  16000\n",
      "idx:  16250\n",
      "idx:  16500\n",
      "idx:  16750\n",
      "idx:  17000\n",
      "idx:  17250\n",
      "idx:  17500\n",
      "idx:  17750\n",
      "idx:  18000\n",
      "idx:  18250\n",
      "idx:  18500\n",
      "idx:  18750\n",
      "idx:  19000\n",
      "idx:  19250\n",
      "idx:  19500\n",
      "idx:  19750\n",
      "idx:  20000\n",
      "idx:  20250\n",
      "idx:  20500\n",
      "idx:  20750\n",
      "idx:  21000\n",
      "idx:  21250\n",
      "idx:  21500\n",
      "idx:  21750\n",
      "idx:  22000\n",
      "idx:  22250\n",
      "idx:  22500\n",
      "idx:  22750\n",
      "idx:  23000\n",
      "Percent of words found in dictionary 99.84%\n",
      "Next Cycle: ###################################################################\n"
     ]
    }
   ],
   "source": [
    "# Delete all BoW Result Matrix:\n",
    "for part in ['URL','Stem']:\n",
    "    loc = f'{save_loc}{part}/BOW/'\n",
    "    print('Delete loc: ',loc)\n",
    "    for f in os.listdir(loc):\n",
    "        os.remove(os.path.join(loc, f))\n",
    "\n",
    "for col in ['url','stem']:\n",
    "    \n",
    "    # read dict:\n",
    "    bodyDict = pd.read_csv(f'{save_loc}Master_dicts/bodyDict_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    body_key_list = list(bodyDict.keys())\n",
    "    titleDict = pd.read_csv(f'{save_loc}Master_dicts/titleDict_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    title_key_list = list(titleDict)\n",
    "\n",
    "    i = 0\n",
    "    word_cnt = 0\n",
    "    S_word_cnt = 0\n",
    "\n",
    "    is_title_skip = False\n",
    "\n",
    "    end = len(df)\n",
    "    # end = 22\n",
    "\n",
    "    for idx, row in df[i:end].iterrows():\n",
    "        # if idx <= cycle_len + 2:\n",
    "        # print(row.id)\n",
    "        try:\n",
    "            word_cnt += len(list(set(row[f'body_{col}'].split())))\n",
    "        except:\n",
    "            row[f'body_{col}'] = 'NA'\n",
    "        try:\n",
    "            word_cnt += len(list(set(row[f'title_{col}'].split())))\n",
    "        except:\n",
    "            row[f'title_{col}'] = 'NA'\n",
    "\n",
    "        if idx%cycle_len == 0:\n",
    "            gc.collect()\n",
    "            print('idx: ',idx)\n",
    "            if idx > 0:\n",
    "                np.savetxt(f'{save_loc}{col}/BOW/id{int(idx/cycle_len)}.csv', id_list, delimiter=\",\", fmt=\"%s\")  \n",
    "                np.savetxt(f'{save_loc}{col}/BOW/y{int(idx/cycle_len)}.csv', y, delimiter=\",\") \n",
    "                # save as sparce array\n",
    "                S_body = scipy.sparse.csr_matrix(body)         \n",
    "                S_title = scipy.sparse.csr_matrix(title)\n",
    "                S_word_cnt += S_body.count_nonzero()\n",
    "                S_word_cnt += S_title.count_nonzero()\n",
    "\n",
    "                # Pickle dump\n",
    "                file = open(f'{save_loc}{col}/BOW/S_body{int(idx/cycle_len)}.pkl','wb')\n",
    "                pickle.dump(S_body,file)\n",
    "                file.close()\n",
    "\n",
    "                file = open(f'{save_loc}{col}/BOW/S_title{int(idx/cycle_len)}.pkl','wb')\n",
    "                pickle.dump(S_title,file)\n",
    "                file.close()\n",
    "\n",
    "            id_list = row['id']\n",
    "            y = row['class']\n",
    "            body = CM.buildRow(row[f'body_{col}'],bodyDict)\n",
    "            title= CM.buildRow(row[f'title_{col}'], titleDict)\n",
    "\n",
    "\n",
    "        else:\n",
    "            id_list = np.vstack([id_list, row['id']])\n",
    "            y = np.vstack([y, row['class']])\n",
    "            body = np.vstack([body, CM.buildRow(row[f'body_{col}'] ,bodyDict)])\n",
    "            title = np.vstack([title, CM.buildRow(row[f'title_{col}'], titleDict)])\n",
    "    #     except:\n",
    "    #         print('Failed for idx: ',idx)\n",
    "\n",
    "    np.savetxt(f'{save_loc}{col}/BOW/id{int(idx/cycle_len) + 1}.csv', id_list, delimiter=\",\", fmt=\"%s\")  \n",
    "    np.savetxt(f'{save_loc}{col}/BOW/y{int(idx/cycle_len) + 1}.csv', y, delimiter=\",\") \n",
    "    S_body = scipy.sparse.csr_matrix(body)\n",
    "    S_title = scipy.sparse.csr_matrix(title)\n",
    "    S_word_cnt += S_body.count_nonzero()\n",
    "    S_word_cnt += S_title.count_nonzero()\n",
    "    # Pickle dump\n",
    "\n",
    "    file = open(f'{save_loc}{col}/BOW/S_body{int(idx/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_body,file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(f'{save_loc}{col}/BOW/S_title{int(idx/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_title,file)  \n",
    "    file.close()\n",
    "\n",
    "    print(f'Percent of words found in dictionary {round(S_word_cnt/word_cnt*100,2)}%')\n",
    "    print('Next Cycle: ###################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/URL/TF/\n",
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/Stem/TF/\n",
      "url\n",
      "Num: 0 idx:  14165 ID:  Fake_0\n",
      "Num: 250 idx:  3050 ID:  Fake_79\n",
      "Num: 500 idx:  2219 ID:  Fake_165\n",
      "Num: 750 idx:  17108 ID:  Fake_254\n",
      "Num: 1000 idx:  9344 ID:  Fake_kag_337\n",
      "Num: 1250 idx:  13825 ID:  True_kag_440\n",
      "Num: 1500 idx:  8208 ID:  Fake_584\n",
      "Num: 1750 idx:  13053 ID:  Fake_758\n",
      "Num: 2000 idx:  3207 ID:  True_kag_916\n",
      "Num: 2250 idx:  10622 ID:  Fake_1093\n",
      "Num: 2500 idx:  12245 ID:  True_kag_1267\n",
      "Num: 2750 idx:  10241 ID:  True_1424\n",
      "Num: 3000 idx:  6687 ID:  True_1595\n",
      "Num: 3250 idx:  21026 ID:  Fake_1758\n",
      "Num: 3500 idx:  1224 ID:  Fake_kag_1924\n",
      "Num: 3750 idx:  21872 ID:  True_2093\n",
      "Num: 4000 idx:  13536 ID:  True_2264\n",
      "Num: 4250 idx:  3814 ID:  Fake_kag_2435\n",
      "Num: 4500 idx:  19919 ID:  True_kag_2607\n",
      "Num: 4750 idx:  22411 ID:  Fake_kag_2769\n",
      "Num: 5000 idx:  16551 ID:  Fake_2937\n",
      "Num: 5250 idx:  10443 ID:  True_3104\n",
      "Num: 5500 idx:  456 ID:  Fake_3262\n",
      "Num: 5750 idx:  9671 ID:  True_3440\n",
      "Num: 6000 idx:  214 ID:  True_3619\n",
      "Num: 6250 idx:  22724 ID:  Fake_3783\n",
      "Num: 6500 idx:  4191 ID:  Fake_kag_3961\n",
      "Num: 6750 idx:  14648 ID:  True_4192\n",
      "Num: 7000 idx:  16598 ID:  Fake_4458\n",
      "Num: 7250 idx:  16436 ID:  True_4732\n",
      "Num: 7500 idx:  13900 ID:  Fake_4980\n",
      "Num: 7750 idx:  19280 ID:  True_5256\n",
      "Num: 8000 idx:  13281 ID:  Fake_5554\n",
      "Num: 8250 idx:  12531 ID:  True_5808\n",
      "Num: 8500 idx:  15884 ID:  Fake_6071\n",
      "Num: 8750 idx:  12763 ID:  True_6355\n",
      "Num: 9000 idx:  13366 ID:  Fake_6619\n",
      "Num: 9250 idx:  4966 ID:  Fake_6902\n",
      "Num: 9500 idx:  12987 ID:  Fake_7187\n",
      "Num: 9750 idx:  7106 ID:  Fake_7457\n",
      "Num: 10000 idx:  17225 ID:  Fake_7729\n",
      "Num: 10250 idx:  19703 ID:  Fake_7998\n",
      "Num: 10500 idx:  3345 ID:  Fake_8294\n",
      "Num: 10750 idx:  1784 ID:  True_8571\n",
      "Num: 11000 idx:  12460 ID:  True_8865\n",
      "Num: 11250 idx:  20818 ID:  True_9173\n",
      "Num: 11500 idx:  10454 ID:  Fake_9447\n",
      "Num: 11750 idx:  14744 ID:  True_9724\n",
      "Num: 12000 idx:  17357 ID:  True_9989\n",
      "Num: 12250 idx:  16297 ID:  True_10266\n",
      "Num: 12500 idx:  23040 ID:  Fake_10523\n",
      "Num: 12750 idx:  17142 ID:  True_10838\n",
      "Num: 13000 idx:  7566 ID:  True_11136\n",
      "Num: 13250 idx:  848 ID:  True_11433\n",
      "Num: 13500 idx:  17281 ID:  True_11755\n",
      "Num: 13750 idx:  22204 ID:  Fake_12061\n",
      "Num: 14000 idx:  4014 ID:  True_12359\n",
      "Num: 14250 idx:  15872 ID:  Fake_12664\n",
      "Num: 14500 idx:  20485 ID:  Fake_12963\n",
      "Num: 14750 idx:  4286 ID:  Fake_13248\n",
      "Num: 15000 idx:  6297 ID:  True_13530\n",
      "Num: 15250 idx:  17098 ID:  True_13796\n",
      "Num: 15500 idx:  20968 ID:  True_14072\n",
      "Num: 15750 idx:  11539 ID:  True_14356\n",
      "Num: 16000 idx:  18914 ID:  True_14638\n",
      "Num: 16250 idx:  7490 ID:  True_14917\n",
      "Num: 16500 idx:  13388 ID:  Fake_15184\n",
      "Num: 16750 idx:  22814 ID:  True_15477\n",
      "Num: 17000 idx:  483 ID:  True_15762\n",
      "Num: 17250 idx:  8792 ID:  Fake_16024\n",
      "Num: 17500 idx:  6368 ID:  Fake_16323\n",
      "Num: 17750 idx:  295 ID:  Fake_16613\n",
      "Num: 18000 idx:  2757 ID:  True_16895\n",
      "Num: 18250 idx:  15281 ID:  True_17163\n",
      "Num: 18500 idx:  19620 ID:  Fake_17424\n",
      "Num: 18750 idx:  18959 ID:  True_17693\n",
      "Num: 19000 idx:  19320 ID:  True_17947\n",
      "Num: 19250 idx:  12155 ID:  Fake_18210\n",
      "Num: 19500 idx:  19477 ID:  Fake_18484\n",
      "Num: 19750 idx:  1212 ID:  True_18754\n",
      "Num: 20000 idx:  1664 ID:  Fake_19041\n",
      "Num: 20250 idx:  6392 ID:  True_19327\n",
      "Num: 20500 idx:  18281 ID:  True_19619\n",
      "Num: 20750 idx:  1476 ID:  Fake_19907\n",
      "Num: 21000 idx:  7187 ID:  True_20183\n",
      "Num: 21250 idx:  5897 ID:  True_20486\n",
      "Num: 21500 idx:  20399 ID:  Fake_20753\n",
      "Num: 21750 idx:  16625 ID:  True_21039\n",
      "Num: 22000 idx:  9368 ID:  True_21330\n",
      "Num: 22250 idx:  11620 ID:  Fake_21830\n",
      "Num: 22500 idx:  14668 ID:  Fake_22447\n",
      "Num: 22750 idx:  22011 ID:  Fake_23012\n",
      "Percent of words found in dictionary 77.87%\n",
      "Next Cycle: ###################################################################\n",
      "stem\n",
      "Num: 0 idx:  14165 ID:  Fake_0\n",
      "Num: 250 idx:  3050 ID:  Fake_79\n",
      "Num: 500 idx:  2219 ID:  Fake_165\n",
      "Num: 750 idx:  17108 ID:  Fake_254\n",
      "Num: 1000 idx:  9344 ID:  Fake_kag_337\n",
      "Num: 1250 idx:  13825 ID:  True_kag_440\n",
      "Num: 1500 idx:  8208 ID:  Fake_584\n",
      "Num: 1750 idx:  13053 ID:  Fake_758\n",
      "Num: 2000 idx:  3207 ID:  True_kag_916\n",
      "Num: 2250 idx:  10622 ID:  Fake_1093\n",
      "Num: 2500 idx:  12245 ID:  True_kag_1267\n",
      "Num: 2750 idx:  10241 ID:  True_1424\n",
      "Num: 3000 idx:  6687 ID:  True_1595\n",
      "Num: 3250 idx:  21026 ID:  Fake_1758\n",
      "Num: 3500 idx:  1224 ID:  Fake_kag_1924\n",
      "Num: 3750 idx:  21872 ID:  True_2093\n",
      "Num: 4000 idx:  13536 ID:  True_2264\n",
      "Num: 4250 idx:  3814 ID:  Fake_kag_2435\n",
      "Num: 4500 idx:  19919 ID:  True_kag_2607\n",
      "Num: 4750 idx:  22411 ID:  Fake_kag_2769\n",
      "Num: 5000 idx:  16551 ID:  Fake_2937\n",
      "Num: 5250 idx:  10443 ID:  True_3104\n",
      "Num: 5500 idx:  456 ID:  Fake_3262\n",
      "Num: 5750 idx:  9671 ID:  True_3440\n",
      "Num: 6000 idx:  214 ID:  True_3619\n",
      "Num: 6250 idx:  22724 ID:  Fake_3783\n",
      "Num: 6500 idx:  4191 ID:  Fake_kag_3961\n",
      "Num: 6750 idx:  14648 ID:  True_4192\n",
      "Num: 7000 idx:  16598 ID:  Fake_4458\n",
      "Num: 7250 idx:  16436 ID:  True_4732\n",
      "Num: 7500 idx:  13900 ID:  Fake_4980\n",
      "Num: 7750 idx:  19280 ID:  True_5256\n",
      "Num: 8000 idx:  13281 ID:  Fake_5554\n",
      "Num: 8250 idx:  12531 ID:  True_5808\n",
      "Num: 8500 idx:  15884 ID:  Fake_6071\n",
      "Num: 8750 idx:  12763 ID:  True_6355\n",
      "Num: 9000 idx:  13366 ID:  Fake_6619\n",
      "Num: 9250 idx:  4966 ID:  Fake_6902\n",
      "Num: 9500 idx:  12987 ID:  Fake_7187\n",
      "Num: 9750 idx:  7106 ID:  Fake_7457\n",
      "Num: 10000 idx:  17225 ID:  Fake_7729\n",
      "Num: 10250 idx:  19703 ID:  Fake_7998\n",
      "Num: 10500 idx:  3345 ID:  Fake_8294\n",
      "Num: 10750 idx:  1784 ID:  True_8571\n",
      "Num: 11000 idx:  12460 ID:  True_8865\n",
      "Num: 11250 idx:  20818 ID:  True_9173\n",
      "Num: 11500 idx:  10454 ID:  Fake_9447\n",
      "Num: 11750 idx:  14744 ID:  True_9724\n",
      "Num: 12000 idx:  17357 ID:  True_9989\n",
      "Num: 12250 idx:  16297 ID:  True_10266\n",
      "Num: 12500 idx:  23040 ID:  Fake_10523\n",
      "Num: 12750 idx:  17142 ID:  True_10838\n",
      "Num: 13000 idx:  7566 ID:  True_11136\n",
      "Num: 13250 idx:  848 ID:  True_11433\n",
      "Num: 13500 idx:  17281 ID:  True_11755\n",
      "Num: 13750 idx:  22204 ID:  Fake_12061\n",
      "Num: 14000 idx:  4014 ID:  True_12359\n",
      "Num: 14250 idx:  15872 ID:  Fake_12664\n",
      "Num: 14500 idx:  20485 ID:  Fake_12963\n",
      "Num: 14750 idx:  4286 ID:  Fake_13248\n",
      "Num: 15000 idx:  6297 ID:  True_13530\n",
      "Num: 15250 idx:  17098 ID:  True_13796\n",
      "Num: 15500 idx:  20968 ID:  True_14072\n",
      "Num: 15750 idx:  11539 ID:  True_14356\n",
      "Num: 16000 idx:  18914 ID:  True_14638\n",
      "Num: 16250 idx:  7490 ID:  True_14917\n",
      "Num: 16500 idx:  13388 ID:  Fake_15184\n",
      "Num: 16750 idx:  22814 ID:  True_15477\n",
      "Num: 17000 idx:  483 ID:  True_15762\n",
      "Num: 17250 idx:  8792 ID:  Fake_16024\n",
      "Num: 17500 idx:  6368 ID:  Fake_16323\n",
      "Num: 17750 idx:  295 ID:  Fake_16613\n",
      "Num: 18000 idx:  2757 ID:  True_16895\n",
      "Num: 18250 idx:  15281 ID:  True_17163\n",
      "Num: 18500 idx:  19620 ID:  Fake_17424\n",
      "Num: 18750 idx:  18959 ID:  True_17693\n",
      "Num: 19000 idx:  19320 ID:  True_17947\n",
      "Num: 19250 idx:  12155 ID:  Fake_18210\n",
      "Num: 19500 idx:  19477 ID:  Fake_18484\n",
      "Num: 19750 idx:  1212 ID:  True_18754\n",
      "Num: 20000 idx:  1664 ID:  Fake_19041\n",
      "Num: 20250 idx:  6392 ID:  True_19327\n",
      "Num: 20500 idx:  18281 ID:  True_19619\n",
      "Num: 20750 idx:  1476 ID:  Fake_19907\n",
      "Num: 21000 idx:  7187 ID:  True_20183\n",
      "Num: 21250 idx:  5897 ID:  True_20486\n",
      "Num: 21500 idx:  20399 ID:  Fake_20753\n",
      "Num: 21750 idx:  16625 ID:  True_21039\n",
      "Num: 22000 idx:  9368 ID:  True_21330\n",
      "Num: 22250 idx:  11620 ID:  Fake_21830\n",
      "Num: 22500 idx:  14668 ID:  Fake_22447\n",
      "Num: 22750 idx:  22011 ID:  Fake_23012\n",
      "Percent of words found in dictionary 99.84%\n",
      "Next Cycle: ###################################################################\n"
     ]
    }
   ],
   "source": [
    "# Build TF Dense Matrix\n",
    "i = 0\n",
    "gc.collect()\n",
    "if i == 0:\n",
    "# Delete all Result Matrix:\n",
    "    for part in ['URL','Stem']:\n",
    "        loc = f'{save_loc}{part}/TF/'\n",
    "        print('Delete loc: ',loc)\n",
    "        for f in os.listdir(loc):\n",
    "            os.remove(os.path.join(loc, f))\n",
    "\n",
    "word_cnt = 0\n",
    "is_title_skip = False\n",
    "\n",
    "\n",
    "end = len(df)\n",
    "# end = 22\n",
    "\n",
    "for col in ['url','stem']:\n",
    "    print(col)\n",
    "    word_cnt = 0\n",
    "    S_word_cnt = 0\n",
    "    gc.collect()\n",
    "    # read IDF dict:\n",
    "    idf_body = pd.read_csv(f'{save_loc}IDFs/body_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    # idf_body_key_list = list(idf_body.keys())\n",
    "    idf_title = pd.read_csv(f'{save_loc}IDFs/title_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    # idf_title_key_list = list(idf_title.keys())\n",
    "    loc = f'{save_loc}TF_dicts/'\n",
    "    body_file_list = [arg[arg.find(col)+len(col)+1:-4] for arg in os.listdir(loc) if (('body' in arg) and (col in arg) and ('User' not in arg) and ('User' not in arg))]\n",
    "    title_file_list = [arg[arg.find(col)+len(col)+1:-4] for arg in os.listdir(loc) if (('title' in arg) and (col in arg) and ('User' not in arg) and ('User' not in arg))]\n",
    "    file_list = list(set(body_file_list + title_file_list))\n",
    "    file_list.sort(key=CM.mySortFunc)\n",
    "\n",
    "    # for idx, row in df.iloc[train_ids][i:end].iterrows():\n",
    "    for num, id_ in enumerate(file_list[i:end]):\n",
    "        gc.collect()\n",
    "        row = df[df.id == id_]\n",
    "        idx = row.index.values[0]\n",
    "        # read TF dict:\n",
    "        try:\n",
    "            try:\n",
    "                bodyDict = pd.read_csv(f'{save_loc}TF_dicts/bodyDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "            except:\n",
    "                try:\n",
    "                    temp = pd.read_csv(f'{save_loc}TF_dicts/bodyDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str})\n",
    "                    bodyDict = {temp[0].values[0]:temp[1].values[0]} \n",
    "                except: \n",
    "                    print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "                    bodyDict = {'Missing':0}\n",
    "\n",
    "            body_key_list = list(bodyDict.keys())\n",
    "\n",
    "            try:\n",
    "                titleDict = pd.read_csv(f'{save_loc}TF_dicts/titleDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "            except:\n",
    "                try:\n",
    "                    temp = pd.read_csv(f'{save_loc}TF_dicts/titleDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str})\n",
    "                    titleDict = {temp[0].values[0]:temp[1].values[0]} \n",
    "                except:\n",
    "                    print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "                    titleDict = {'Missing':0}\n",
    "\n",
    "            title_key_list = list(titleDict)\n",
    "\n",
    "            # try:\n",
    "            word_cnt += len(list(set(row[f'body_{col}'].values[0].split())))\n",
    "            try:\n",
    "                word_cnt += len(list(set(row[f'title_{col}'].values[0].split())))\n",
    "            except:\n",
    "                row[f'title_{col}'] = 'NA'\n",
    "\n",
    "            # print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "            if num%cycle_len == 0:\n",
    "                gc.collect()\n",
    "                time.sleep(60)\n",
    "                print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "                if num > 0:\n",
    "                    np.savetxt(f'{save_loc}{col}/TF/id{int(num/cycle_len)}.csv', id_list, delimiter=\",\", fmt=\"%s\") \n",
    "                    np.savetxt(f'{save_loc}{col}/TF/y{int(num/cycle_len)}.csv', y, delimiter=\",\", fmt=\"%s\") \n",
    "                    # save as sparce array\n",
    "                    S_body = scipy.sparse.csr_matrix(body)         \n",
    "                    S_title = scipy.sparse.csr_matrix(title)\n",
    "                    S_word_cnt += S_body.count_nonzero()\n",
    "                    S_word_cnt += S_title.count_nonzero()\n",
    "\n",
    "\n",
    "                    file = open(f'{save_loc}{col}/TF/S_body{int(num/cycle_len)}.pkl','wb')\n",
    "                    pickle.dump(S_body,file)\n",
    "                    file.close()\n",
    "\n",
    "                    file = open(f'{save_loc}{col}/TF/S_title{int(num/cycle_len)}.pkl','wb')\n",
    "                    pickle.dump(S_title,file)\n",
    "                    file.close()\n",
    "\n",
    "                id_list = row['id'].values[0]\n",
    "                y = row['class'].values[0]\n",
    "                body = CM.build_TF(bodyDict, idf_body)\n",
    "                title= CM.build_TF(titleDict, idf_title)\n",
    "\n",
    "            else:\n",
    "                id_list = np.vstack([id_list, row['id'].values[0]])\n",
    "                y = np.vstack([y, row['class'].values[0]])\n",
    "                body = np.vstack([body, CM.build_TF(bodyDict, idf_body)])\n",
    "                title = np.vstack([title, CM.build_TF(titleDict, idf_title)])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    np.savetxt(f'{save_loc}{col}/TF/id{int(num/cycle_len) + 1}.csv', id_list, delimiter=\",\", fmt=\"%s\") \n",
    "    np.savetxt(f'{save_loc}{col}/TF/y{int(num/cycle_len) + 1}.csv', y, delimiter=\",\", fmt=\"%s\") \n",
    "\n",
    "    S_body = scipy.sparse.csr_matrix(body)\n",
    "    S_title = scipy.sparse.csr_matrix(title)\n",
    "    S_word_cnt += S_body.count_nonzero()\n",
    "    S_word_cnt += S_title.count_nonzero()\n",
    "    # Pickle dump\n",
    "\n",
    "    file = open(f'{save_loc}{col}/TF/S_body{int(num/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_body,file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(f'{save_loc}{col}/TF/S_title{int(num/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_title,file)  \n",
    "    file.close()\n",
    "\n",
    "    print(f'Percent of words found in dictionary {round(S_word_cnt/word_cnt*100,2)}%')\n",
    "    print('Next Cycle: ###################################################################')     \n",
    "# Num: 0 idx:  1495 ID:  Fake_6872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_ = pd.read_csv(loc, header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "# pd.read_csv(loc, header=None, dtype= {0:str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/Raw/LIWC/Stemmed_All/TF_dicts/'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/URL/TFIDF/\n",
      "Delete loc:  dataset/Raw/LIWC/Stemmed_All/Stem/TFIDF/\n",
      "url\n",
      "Num: 0 idx:  14165 ID:  Fake_0\n",
      "Num: 250 idx:  3050 ID:  Fake_79\n",
      "Num: 500 idx:  2219 ID:  Fake_165\n",
      "Num: 750 idx:  17108 ID:  Fake_254\n",
      "Num: 1000 idx:  9344 ID:  Fake_kag_337\n",
      "Num: 1250 idx:  13825 ID:  True_kag_440\n",
      "Num: 1500 idx:  8208 ID:  Fake_584\n",
      "Num: 1750 idx:  13053 ID:  Fake_758\n",
      "Num: 2000 idx:  3207 ID:  True_kag_916\n",
      "Num: 2250 idx:  10622 ID:  Fake_1093\n",
      "Num: 2500 idx:  12245 ID:  True_kag_1267\n",
      "Num: 2750 idx:  10241 ID:  True_1424\n",
      "Num: 3000 idx:  6687 ID:  True_1595\n",
      "Num: 3250 idx:  21026 ID:  Fake_1758\n",
      "Num: 3500 idx:  1224 ID:  Fake_kag_1924\n",
      "Num: 3750 idx:  21872 ID:  True_2093\n",
      "Num: 4000 idx:  13536 ID:  True_2264\n",
      "Num: 4250 idx:  3814 ID:  Fake_kag_2435\n",
      "Num: 4500 idx:  19919 ID:  True_kag_2607\n",
      "Num: 4750 idx:  22411 ID:  Fake_kag_2769\n",
      "Num: 5000 idx:  16551 ID:  Fake_2937\n",
      "Num: 5250 idx:  10443 ID:  True_3104\n",
      "Num: 5500 idx:  456 ID:  Fake_3262\n",
      "Num: 5750 idx:  9671 ID:  True_3440\n",
      "Num: 6000 idx:  214 ID:  True_3619\n",
      "Num: 6250 idx:  22724 ID:  Fake_3783\n",
      "Num: 6500 idx:  4191 ID:  Fake_kag_3961\n",
      "Num: 6750 idx:  14648 ID:  True_4192\n",
      "Num: 7000 idx:  16598 ID:  Fake_4458\n",
      "Num: 7250 idx:  16436 ID:  True_4732\n",
      "Num: 7500 idx:  13900 ID:  Fake_4980\n",
      "Num: 7750 idx:  19280 ID:  True_5256\n",
      "Num: 8000 idx:  13281 ID:  Fake_5554\n",
      "Num: 8250 idx:  12531 ID:  True_5808\n",
      "Num: 8500 idx:  15884 ID:  Fake_6071\n",
      "Num: 8750 idx:  12763 ID:  True_6355\n",
      "Num: 9000 idx:  13366 ID:  Fake_6619\n",
      "Num: 9250 idx:  4966 ID:  Fake_6902\n",
      "Num: 9500 idx:  12987 ID:  Fake_7187\n",
      "Num: 9750 idx:  7106 ID:  Fake_7457\n",
      "Num: 10000 idx:  17225 ID:  Fake_7729\n",
      "Num: 10250 idx:  19703 ID:  Fake_7998\n",
      "Num: 10500 idx:  3345 ID:  Fake_8294\n",
      "Num: 10750 idx:  1784 ID:  True_8571\n",
      "Num: 11000 idx:  12460 ID:  True_8865\n",
      "Num: 11250 idx:  20818 ID:  True_9173\n",
      "Num: 11500 idx:  10454 ID:  Fake_9447\n",
      "Num: 11750 idx:  14744 ID:  True_9724\n",
      "Num: 12000 idx:  17357 ID:  True_9989\n",
      "Num: 12250 idx:  16297 ID:  True_10266\n",
      "Num: 12500 idx:  23040 ID:  Fake_10523\n",
      "Num: 12750 idx:  17142 ID:  True_10838\n",
      "Num: 13000 idx:  7566 ID:  True_11136\n",
      "Num: 13250 idx:  848 ID:  True_11433\n",
      "Num: 13500 idx:  17281 ID:  True_11755\n",
      "Num: 13750 idx:  22204 ID:  Fake_12061\n",
      "Num: 14000 idx:  4014 ID:  True_12359\n",
      "Num: 14250 idx:  15872 ID:  Fake_12664\n",
      "Num: 14500 idx:  20485 ID:  Fake_12963\n",
      "Num: 14750 idx:  4286 ID:  Fake_13248\n",
      "Num: 15000 idx:  6297 ID:  True_13530\n",
      "Num: 15250 idx:  17098 ID:  True_13796\n",
      "Num: 15500 idx:  20968 ID:  True_14072\n",
      "Num: 15750 idx:  11539 ID:  True_14356\n",
      "Num: 16000 idx:  18914 ID:  True_14638\n",
      "Num: 16250 idx:  7490 ID:  True_14917\n",
      "Num: 16500 idx:  13388 ID:  Fake_15184\n",
      "Num: 16750 idx:  22814 ID:  True_15477\n",
      "Num: 17000 idx:  483 ID:  True_15762\n",
      "Num: 17250 idx:  8792 ID:  Fake_16024\n",
      "Num: 17500 idx:  6368 ID:  Fake_16323\n",
      "Num: 17750 idx:  295 ID:  Fake_16613\n",
      "Num: 18000 idx:  2757 ID:  True_16895\n",
      "Num: 18250 idx:  15281 ID:  True_17163\n",
      "Num: 18500 idx:  19620 ID:  Fake_17424\n",
      "Num: 18750 idx:  18959 ID:  True_17693\n",
      "Num: 19000 idx:  19320 ID:  True_17947\n",
      "Num: 19250 idx:  12155 ID:  Fake_18210\n",
      "Num: 19500 idx:  19477 ID:  Fake_18484\n",
      "Num: 19750 idx:  1212 ID:  True_18754\n",
      "Num: 20000 idx:  1664 ID:  Fake_19041\n",
      "Num: 20250 idx:  6392 ID:  True_19327\n",
      "Num: 20500 idx:  18281 ID:  True_19619\n",
      "Num: 20750 idx:  1476 ID:  Fake_19907\n",
      "Num: 21000 idx:  7187 ID:  True_20183\n",
      "Num: 21250 idx:  5897 ID:  True_20486\n",
      "Num: 21500 idx:  20399 ID:  Fake_20753\n",
      "Num: 21750 idx:  16625 ID:  True_21039\n",
      "Num: 22000 idx:  9368 ID:  True_21330\n",
      "Num: 22250 idx:  11620 ID:  Fake_21830\n",
      "Num: 22500 idx:  14668 ID:  Fake_22447\n",
      "Num: 22750 idx:  22011 ID:  Fake_23012\n",
      "Percent of words found in dictionary 77.87%\n",
      "Next Cycle: ###################################################################\n",
      "stem\n",
      "Num: 0 idx:  14165 ID:  Fake_0\n",
      "Num: 250 idx:  3050 ID:  Fake_79\n",
      "Num: 500 idx:  2219 ID:  Fake_165\n",
      "Num: 750 idx:  17108 ID:  Fake_254\n",
      "Num: 1000 idx:  9344 ID:  Fake_kag_337\n",
      "Num: 1250 idx:  13825 ID:  True_kag_440\n",
      "Num: 1500 idx:  8208 ID:  Fake_584\n",
      "Num: 1750 idx:  13053 ID:  Fake_758\n",
      "Num: 2000 idx:  3207 ID:  True_kag_916\n",
      "Num: 2250 idx:  10622 ID:  Fake_1093\n",
      "Num: 2500 idx:  12245 ID:  True_kag_1267\n",
      "Num: 2750 idx:  10241 ID:  True_1424\n",
      "Num: 3000 idx:  6687 ID:  True_1595\n",
      "Num: 3250 idx:  21026 ID:  Fake_1758\n",
      "Num: 3500 idx:  1224 ID:  Fake_kag_1924\n",
      "Num: 3750 idx:  21872 ID:  True_2093\n",
      "Num: 4000 idx:  13536 ID:  True_2264\n",
      "Num: 4250 idx:  3814 ID:  Fake_kag_2435\n",
      "Num: 4500 idx:  19919 ID:  True_kag_2607\n",
      "Num: 4750 idx:  22411 ID:  Fake_kag_2769\n",
      "Num: 5000 idx:  16551 ID:  Fake_2937\n",
      "Num: 5250 idx:  10443 ID:  True_3104\n",
      "Num: 5500 idx:  456 ID:  Fake_3262\n",
      "Num: 5750 idx:  9671 ID:  True_3440\n",
      "Num: 6000 idx:  214 ID:  True_3619\n",
      "Num: 6250 idx:  22724 ID:  Fake_3783\n",
      "Num: 6500 idx:  4191 ID:  Fake_kag_3961\n",
      "Num: 6750 idx:  14648 ID:  True_4192\n",
      "Num: 7000 idx:  16598 ID:  Fake_4458\n",
      "Num: 7250 idx:  16436 ID:  True_4732\n",
      "Num: 7500 idx:  13900 ID:  Fake_4980\n",
      "Num: 7750 idx:  19280 ID:  True_5256\n",
      "Num: 8000 idx:  13281 ID:  Fake_5554\n",
      "Num: 8250 idx:  12531 ID:  True_5808\n",
      "Num: 8500 idx:  15884 ID:  Fake_6071\n",
      "Num: 8750 idx:  12763 ID:  True_6355\n",
      "Num: 9000 idx:  13366 ID:  Fake_6619\n",
      "Num: 9250 idx:  4966 ID:  Fake_6902\n",
      "Num: 9500 idx:  12987 ID:  Fake_7187\n",
      "Num: 9750 idx:  7106 ID:  Fake_7457\n",
      "Num: 10000 idx:  17225 ID:  Fake_7729\n",
      "Num: 10250 idx:  19703 ID:  Fake_7998\n",
      "Num: 10500 idx:  3345 ID:  Fake_8294\n",
      "Num: 10750 idx:  1784 ID:  True_8571\n",
      "Num: 11000 idx:  12460 ID:  True_8865\n",
      "Num: 11250 idx:  20818 ID:  True_9173\n",
      "Num: 11500 idx:  10454 ID:  Fake_9447\n",
      "Num: 11750 idx:  14744 ID:  True_9724\n",
      "Num: 12000 idx:  17357 ID:  True_9989\n",
      "Num: 12250 idx:  16297 ID:  True_10266\n",
      "Num: 12500 idx:  23040 ID:  Fake_10523\n",
      "Num: 12750 idx:  17142 ID:  True_10838\n",
      "Num: 13000 idx:  7566 ID:  True_11136\n",
      "Num: 13250 idx:  848 ID:  True_11433\n",
      "Num: 13500 idx:  17281 ID:  True_11755\n",
      "Num: 13750 idx:  22204 ID:  Fake_12061\n",
      "Num: 14000 idx:  4014 ID:  True_12359\n",
      "Num: 14250 idx:  15872 ID:  Fake_12664\n",
      "Num: 14500 idx:  20485 ID:  Fake_12963\n",
      "Num: 14750 idx:  4286 ID:  Fake_13248\n",
      "Num: 15000 idx:  6297 ID:  True_13530\n",
      "Num: 15250 idx:  17098 ID:  True_13796\n",
      "Num: 15500 idx:  20968 ID:  True_14072\n",
      "Num: 15750 idx:  11539 ID:  True_14356\n",
      "Num: 16000 idx:  18914 ID:  True_14638\n",
      "Num: 16250 idx:  7490 ID:  True_14917\n",
      "Num: 16500 idx:  13388 ID:  Fake_15184\n",
      "Num: 16750 idx:  22814 ID:  True_15477\n",
      "Num: 17000 idx:  483 ID:  True_15762\n",
      "Num: 17250 idx:  8792 ID:  Fake_16024\n",
      "Num: 17500 idx:  6368 ID:  Fake_16323\n",
      "Num: 17750 idx:  295 ID:  Fake_16613\n",
      "Num: 18000 idx:  2757 ID:  True_16895\n",
      "Num: 18250 idx:  15281 ID:  True_17163\n",
      "Num: 18500 idx:  19620 ID:  Fake_17424\n",
      "Num: 18750 idx:  18959 ID:  True_17693\n",
      "Num: 19000 idx:  19320 ID:  True_17947\n",
      "Num: 19250 idx:  12155 ID:  Fake_18210\n",
      "Num: 19500 idx:  19477 ID:  Fake_18484\n",
      "Num: 19750 idx:  1212 ID:  True_18754\n",
      "Num: 20000 idx:  1664 ID:  Fake_19041\n",
      "Num: 20250 idx:  6392 ID:  True_19327\n",
      "Num: 20500 idx:  18281 ID:  True_19619\n",
      "Num: 20750 idx:  1476 ID:  Fake_19907\n",
      "Num: 21000 idx:  7187 ID:  True_20183\n",
      "Num: 21250 idx:  5897 ID:  True_20486\n",
      "Num: 21500 idx:  20399 ID:  Fake_20753\n",
      "Num: 21750 idx:  16625 ID:  True_21039\n",
      "Num: 22000 idx:  9368 ID:  True_21330\n",
      "Num: 22250 idx:  11620 ID:  Fake_21830\n",
      "Num: 22500 idx:  14668 ID:  Fake_22447\n",
      "Num: 22750 idx:  22011 ID:  Fake_23012\n",
      "Percent of words found in dictionary 99.84%\n",
      "Next Cycle: ###################################################################\n"
     ]
    }
   ],
   "source": [
    "# Build TFIDF Dense Matrix\n",
    "i = 0\n",
    "gc.collect()\n",
    "if i == 0:\n",
    "    #Delete all TFIDF Result Matrix:\n",
    "    for part in ['URL','Stem']:\n",
    "        loc = f'{save_loc}{part}/TFIDF/'\n",
    "        print('Delete loc: ',loc)\n",
    "        for f in os.listdir(loc):\n",
    "            os.remove(os.path.join(loc, f))\n",
    "        \n",
    "\n",
    "word_cnt = 0\n",
    "is_title_skip = False\n",
    "\n",
    "\n",
    "end = len(df)\n",
    "# end = 22\n",
    "\n",
    "for col in ['url','stem']:\n",
    "    print(col)\n",
    "    word_cnt = 0\n",
    "    S_word_cnt = 0\n",
    "    gc.collect()\n",
    "    # read IDF dict:\n",
    "    idf_body = pd.read_csv(f'{save_loc}IDFs/body_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    # idf_body_key_list = list(idf_body.keys())\n",
    "    idf_title = pd.read_csv(f'{save_loc}IDFs/title_{col}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "    # idf_title_key_list = list(idf_title.keys())\n",
    "    loc = f'{save_loc}TF_dicts/'\n",
    "    body_file_list = [arg[arg.find(col)+len(col)+1:-4] for arg in os.listdir(loc) if (('body' in arg) and (col in arg) and ('User' not in arg) and ('User' not in arg))]\n",
    "    title_file_list = [arg[arg.find(col)+len(col)+1:-4] for arg in os.listdir(loc) if (('title' in arg) and (col in arg) and ('User' not in arg) and ('User' not in arg))]\n",
    "    file_list = list(set(body_file_list + title_file_list))\n",
    "    file_list.sort(key=CM.mySortFunc)\n",
    "\n",
    "    # for idx, row in df.iloc[train_ids][i:end].iterrows():\n",
    "    for num, id_ in enumerate(file_list[i:end]):\n",
    "        gc.collect()\n",
    "        row = df[df.id == id_]\n",
    "        idx = row.index.values[0]\n",
    "        # read TF dict:\n",
    "        \n",
    "        try:\n",
    "            bodyDict = pd.read_csv(f'{save_loc}TF_dicts/bodyDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "        except:\n",
    "            try:\n",
    "                temp = pd.read_csv(f'{save_loc}TF_dicts/bodyDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str})\n",
    "                bodyDict = {temp[0].values[0]:temp[1].values[0]} \n",
    "            except:   \n",
    "                bodyDict = {'Missing':0}\n",
    "        \n",
    "        body_key_list = list(bodyDict.keys())\n",
    "        \n",
    "        try:\n",
    "            titleDict = pd.read_csv(f'{save_loc}TF_dicts/titleDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str}).set_index(0).squeeze().to_dict()\n",
    "        except:\n",
    "            try:\n",
    "                temp = pd.read_csv(f'{save_loc}TF_dicts/titleDict_{col}_{row.id.values[0]}.csv', header=None, dtype= {0:str})\n",
    "                titleDict = {temp[0].values[0]:temp[1].values[0]} \n",
    "            except:\n",
    "                titleDict = {'Missing':0}\n",
    "        title_key_list = list(titleDict)\n",
    "            \n",
    "        # try:\n",
    "        word_cnt += len(list(set(row[f'body_{col}'].values[0].split())))\n",
    "        try:\n",
    "            word_cnt += len(list(set(row[f'title_{col}'].values[0].split())))\n",
    "        except:\n",
    "            row[f'title_{col}'] = 'NA'\n",
    "\n",
    "        # print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "        if num%cycle_len == 0:\n",
    "            gc.collect()\n",
    "            time.sleep(60)\n",
    "            print('Num:',num, 'idx: ',idx, 'ID: ',id_)\n",
    "            if num > 0:\n",
    "                np.savetxt(f'{save_loc}{col}/TFIDF/id{int(num/cycle_len)}.csv', id_list, delimiter=\",\", fmt=\"%s\")  \n",
    "                np.savetxt(f'{save_loc}{col}/TFIDF/y{int(num/cycle_len)}.csv', y, delimiter=\",\") \n",
    "                # save as sparce array\n",
    "                S_body = scipy.sparse.csr_matrix(body)         \n",
    "                S_title = scipy.sparse.csr_matrix(title)\n",
    "                S_word_cnt += S_body.count_nonzero()\n",
    "                S_word_cnt += S_title.count_nonzero()\n",
    "\n",
    "                # Pickle dump\n",
    "                file = open(f'{save_loc}{col}/TFIDF/S_body{int(num/cycle_len)}.pkl','wb')\n",
    "                pickle.dump(S_body,file)\n",
    "                file.close()\n",
    "\n",
    "                file = open(f'{save_loc}{col}/TFIDF/S_title{int(num/cycle_len)}.pkl','wb')\n",
    "                pickle.dump(S_title,file)\n",
    "                file.close()\n",
    "\n",
    "            \n",
    "            id_list = row['id'].values[0]\n",
    "            y = row['class'].values[0]\n",
    "            body = CM.build_TFIDF(bodyDict, idf_body)\n",
    "            title= CM.build_TFIDF(titleDict, idf_title)\n",
    "\n",
    "        else:\n",
    "            id_list = np.vstack([id_list, row['id'].values[0]])\n",
    "            y = np.vstack([y, row['class'].values[0]])\n",
    "            body = np.vstack([body, CM.build_TFIDF(bodyDict, idf_body)])\n",
    "            title = np.vstack([title, CM.build_TFIDF(titleDict, idf_title)])\n",
    "\n",
    "\n",
    "    np.savetxt(f'{save_loc}{col}/TFIDF/id{int(num/cycle_len) + 1}.csv', id_list, delimiter=\",\", fmt=\"%s\")  \n",
    "    np.savetxt(f'{save_loc}{col}/TFIDF/y{int(num/cycle_len) + 1}.csv', y, delimiter=\",\") \n",
    "    S_body = scipy.sparse.csr_matrix(body)\n",
    "    S_title = scipy.sparse.csr_matrix(title)\n",
    "    S_word_cnt += S_body.count_nonzero()\n",
    "    S_word_cnt += S_title.count_nonzero()\n",
    "    # Pickle dump\n",
    "    file = open(f'{save_loc}{col}/TFIDF/S_body{int(num/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_body,file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(f'{save_loc}{col}/TFIDF/S_title{int(num/cycle_len) + 1}.pkl','wb')\n",
    "    pickle.dump(S_title,file)  \n",
    "    file.close()\n",
    "\n",
    "    print(f'Percent of words found in dictionary {round(S_word_cnt/word_cnt*100,2)}%')\n",
    "    print('Next Cycle: ###################################################################')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook took 1265 min to run.\n"
     ]
    }
   ],
   "source": [
    "Notebook_end = time.time()\n",
    "Total_Notebook_Time = round((Notebook_end - Notebook_start)/60.0,)\n",
    "print(f'This notebook took {Total_Notebook_Time} min to run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 72555\n"
     ]
    }
   ],
   "source": [
    "print(len(bodyDict.keys()), len(idf_body.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22946"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
